%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Poster [182] for Facing the Multi-Core Challenge conference Stuttgart 2012
% http://www.multicore-challenge.org
% 
% Authors: P. Bialas, J. Kowal, A. Strzelecki
% http://www.uj.edu.pl/web/zaklad-technologii-gier
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{a0poster}

\newcommand{\vphi}{\vec{\varphi}}
\newcommand{\vi}{{\vec{i}}}
\newcommand{\vj}{{\vec{j}}}
\newcommand{\vmu}{\vec{\mu}}

\usepackage{tikzposter}

\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows, shapes, backgrounds}
\usetikzlibrary{shadows}

\renewcommand{\margin}{2}
% \renewcommand{\blockspacing}{2}
% \renewcommand{\blocktitlehight}{3}
% \renewcommand{\colnumber}{3}
% \renewcommand{\authorshift}{10}

\definecolor{colorone}{HTML}{006633} % default 116699
\definecolor{colortwo}{HTML}{DDFF99} % default CCCCCC
% \definecolor{colorthree}{HTML}{991111} % default 991111

% size of the document
\usepackage[margin=\margin cm, paperwidth=84.1cm, paperheight=118.9cm]{geometry}

% changing the fonts
% \usepackage[light,math]{iwona}
% \usepackage[light]{kurier}
\usepackage{lmodern}
\renewcommand*\familydefault{\sfdefault}
% \usepackage{cmbright}
\usepackage[T1]{fontenc}

% add your packages here
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{bold-extra}

\lstdefinelanguage
  [GCC]{C++}
  []{C++}
{
  morekeywords={[2]__attribute__},%
  morekeywords={[3]vector_size, vec_t, uvec_t, fvec_t}%
}

\lstset{
  language={[GCC]C++},%
  basicstyle=\small\ttfamily,%
  keywordstyle=\ttfamily\bfseries,%
  keywordstyle=[2]\bfseries\color{darkgray},%
  keywordstyle=[3]\itshape,%
  commentstyle=\color{gray},%
  stringstyle=\color{green},%
% numbers=left,%
% numberstyle=\tiny,%
  stepnumber=1,%
  numbersep=5pt,%
% backgroundcolor=\color{lightlightgray},%
% frame=single,%
% rulecolor=\color{lightgray},%
% frameshape={RYRYNYYYY}{yny}{yny}{RYRYNYYYY}, 
  tabsize=2,%
  captionpos=b,%
  breaklines=true,%
  breakatwhitespace=false,%
  showspaces=false,%
  showtabs=false,%
  columns=flexible%
}

% \titlepic{}

\title{GPU-accelerated and CPU SIMD Optimized \\
Monte Carlo Simulation of $\phi^4$ Model}

\author{Piotr Bialas \and Jakub Kowal \and Adam Strzelecki \\
Faculty of Physics, Astronomy and Applied Computer Science \\
Jagiellonian University \\
ul. Reymonta 4, 30-059 Krakow, Poland \\
\\
\texttt{www.uj.edu.pl/web/zaklad-technologii-gier}
}

% defined in tikzposter, but now we want to include logo
\renewcommand{\titleblock}[2][($0.5*(0, \paperheight)-(0, \margin)$)] {%
\node[draw, frame, color=colortwo, fill=white, anchor=north, text=black]
  (title) at #1 {%
  \begin{minipage}{1cm}
    \includegraphics{./UJ-logo.pdf}
  \end{minipage}
  \begin{minipage}{#2 cm}
    \maketitle
  \end{minipage}
  };
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} \AddToShipoutPicture{\BackgroundPicture}

\noindent % to have the picture right in the center
\begin{tikzpicture}
\initializesizeandshifts
\titleblock{50}

\begin{blocknode}[($(title.south)-(xshift)-(yshift)$)]{Introduction}

This contribution is concerned with an efficient implementation of the
Monte-Carlo simulations of the $\varphi^4$ model\cite{parisi}. The problem is
defined as follows: having a vector field $\vphi$ defined on a regular
rectangular two or three dimensional grid we want to generate the field
configurations with probability proportional to $\exp(-H(\vphi))$ where
\begin{equation}\label{eq:ham}\begin{split}
H(\varphi)&=\sum_{\vi}\Biggl(
\frac{1}{2}\sum_{\mu=1}^d(\vphi_{\vi+\hat{\mu}}-\vphi_{\vi})^2
+\frac{\mu^2}{2}|\vphi_{\vi}|^2+
\frac{g}{24}(|\vphi_{\vi}|^2)^2\\
&\phantom{=\sum_{\vi}\bigl(}+
\frac{1}{2\Lambda}\Bigl(\sum_{\mu=1}^d(\vphi_{\vi+\hat{\mu}}-2\vphi_{\vi}+\vphi_{\vi-\vmu})\Bigr)^2\Biggr).
\end{split}
\end{equation}
In the above expression $\vi$ denotes the grid point,
$\vphi_\vi$ denotes the $N$ component vector at the grid point $\vi$
 and $\vmu$ denotes the unit displacement on the grid in direction $\mu$.

The actual  generation is done by the mean of the Metropolis  algorithm as follows: for a given lattice point $\vj$ we produce a new field
\begin{equation}
\widetilde{\vphi}_\vj=\vphi_\vj+\vec{\epsilon}
\end{equation}
where $\epsilon$ is some random vector. The we calculate the difference
$\Delta H = H(\widetilde{\vphi})-H(\vphi)$.
We then replace $\vphi_\vj$ with $\widetilde{\vphi}_\vj$ with the probability
\begin{equation}
P=\max\left\{1,\exp(-\Delta H)\right\}.
\end{equation}
The crucial feature of this algorithm is that the $\Delta H$ depends
only on the immediate neighborhood of the point $\vj$. In our case
because of the last term in \eqref{eq:ham} this neighborhood is
extended compared to usual nearest neighbors (see figure~\ref{fig:nn} (Right)).

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{Parallelization}

While model is inherently parallelizable, grid points that lie in the same
neighborhood cannot be updated together. Taking into account a larger
neighborhood means that a simple checkerboard decomposition pattern cannot be
used and we have devised a new grid decomposition scheme. \\

\begin{center}

\def\bksize{16}
\def\bkcount{4}

\begin{tikzpicture}[scale=.255]

\pgfmathtruncatemacro\lcsize{\bksize * \bkcount}
\pgfmathtruncatemacro\lcsizem{\lcsize - 1}
\pgfmathtruncatemacro\lcsizeb{\lcsize - \bksize}
\pgfmathtruncatemacro\bksizea{\bksize - 2}
\pgfmathtruncatemacro\bksizeb{\bksize - 1}
\pgfmathtruncatemacro\bksizec{\bksize * 2}
\pgfmathtruncatemacro\bksizem{\bksizec - 1}
\pgfmathtruncatemacro\bksized{\bksizec + 1}

\draw[very thin, gray] (-.5, -.5) grid (\lcsize - .5, \lcsize - .5);

\foreach \x in {0, \bksize, ..., \lcsizeb}
  \foreach \y in {0, \bksize, ..., \lcsizeb} {
    \pgfmathtruncatemacro\n{Mod(\x/\bksize, 2)+2 * Mod(\y/\bksize, 2)+1}
    \draw[colorone, xshift=\x cm, yshift=\y cm, rounded corners=2pt]
      (-.4, -.4) rectangle
      node[red, fill opacity=.5, anchor=center, font=\Huge\bfseries\sffamily]
      {\n} (\bksize - .6, \bksize - .6);
  }

\foreach \x in {0, ..., \lcsizem}
  \foreach \y in {0, ..., \lcsizem}
    \path[draw=black, fill=white] (\x, \y) circle(.25);

\foreach \x in {\bksize, ..., \bksizem}
  \foreach \y in {\bksize, ..., \bksizem}
    \fill[black] (\x, \y) circle(.25);

\foreach \x in {\bksize, ..., \bksizem}
  \foreach \y in {\bksizea, \bksizeb, \bksizec, \bksized} {
    \fill[gray] (\x, \y) circle(.25);
    \fill[gray] (\y, \x) circle(.25);
  }

\foreach \p in {(\bksizeb, \bksizeb), (\bksizeb, \bksizec),
                (\bksizec, \bksizeb), (\bksizec, \bksizec)}
  \fill[gray] \p circle(.25);

\end{tikzpicture}
\begin{tikzpicture}[scale=.905]

\clip[rounded corners=0cm] (-1, -1) rectangle (\bksize + 1, \bksize + 1);

\draw[help lines] (-1, -1) grid (\bksize + 1, \bksize + 1);

% lattice points
\foreach \x in {0, ..., \bksize}
  \foreach \y in {0, ..., \bksize}
    \draw (\x, \y) circle (.16);

% neighborhood
\def\neighborhood{--
  ++( 0,  -.5)-- ++( 1,  0)-- ++( 0, -.5 )-- ++(0, -.5)-- ++( 1, 0  )--
  ++( 0, -1.0)-- ++( 1,  0)-- ++( 0,  1.0)-- ++(1, 0  )-- ++( 0, 1.0)--
  ++( 1,  0  )-- ++( 0,  1)-- ++(-1,  0  )-- ++(0, 1  )-- ++(-1, 0  )--
  ++( 0,  1  )-- ++(-1,  0)-- ++( 0, -1  )--
  ++(-1,  0  )-- ++( 0, -1)-- ++(-1,  0  )-- ++(0, -.6)}

\pgfmathtruncatemacro\lchalf{\bksize / 2}

\foreach \p in {0, 1} % pass 1 - neighbourhoods, 2 - points
  \foreach \x in {0, ..., \bksize}
    \foreach \y in {0, ..., \bksize} {
      \pgfmathtruncatemacro\my{Mod(\y, 2)}
      \ifnum \my = 0
        \pgfmathtruncatemacro\ny{Mod(\y, 4)}
        \ifnum \ny = 2
          \pgfmathtruncatemacro\mx{Mod(\x+2, 4)}
        \else
          \pgfmathtruncatemacro\mx{Mod(\x, 4)}
        \fi
        \ifnum \mx = 0
          \ifnum \p = 0
            \def\c{0}
            \ifnum \x = \lchalf
              \ifnum \y = \lchalf
                \def\c{1}
              \fi
            \fi
            \ifnum \c = 0
              \path[draw=black, fill=black, fill opacity=.2,
                    rounded corners=.25cm, shorten >=2pt]
                (\x - 2.5, \y) \neighborhood;
            \fi
          \else
            \fill[red] (\x, \y) circle(.16);
          \fi
        \fi
      \fi
    }

% main center neighborhood
\path[draw=red, fill=red, fill opacity=.4, rounded corners=.25cm, shorten >=2pt, 
      line width=.75mm, cap=round, join=round]
      (\lchalf - 2.5, \lchalf) \neighborhood;

\end{tikzpicture}

\end{center}

\textbf{Figure 1} (Left) The partition of the lattice into blocks. The blocks
with same number are processed in parallel by different thread blocks. \\

\textbf{Figure 2} (Right) Partitioning of the blocks into disjoint sublattices.
Thick black line denotes the neighborhood used in updating the center point.
Black points are processes in parallel by different threads of the same thread
block.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{Tausworthe RNG}

\lstinputlisting{MC3-tausworthe.cpp}

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}[($(title.south)+(xshift)-(yshift)$)]{GPU Implementation}

On GPU we adopt the hierarchical scheme from ref.~\cite{weigel} suitably
modified to account for bigger neighborhood. We first divide the whole lattice
in blocks of $32\times 32$ points. Then we start a kernel that process every
forth block. Each block is assigned to a block of 128 threads. First we fetch
the values of the fields from global to shared memory (including border
points). After that each thread updates one point from the first partition.
Then after synchronization, next partition is updated and so on. After
processing all eight(2D) or 16 (3D) partitions the kernel writes the shared
memory back into global and new kernel is started processing next batch of
blocks. Altogether in this way we managed to achieve $0.13$ nanoseconds for
single lattice field update on \emph{NVIDIA GTX 470}, reaching around $430$
Gflops that is $40\%$ of $1088$ Gflops peak performance of this device.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{CPU Implementation}

In order to provide unbiased CPU vs GPU speed up results we provide
multithreaded vectorized CPU implementation. It uses OpenMP for parallel
execution, SSE/AVX and compiler vector extensions for vectorization. This
implementation does mimic GPU SIMT execution model. The SIMD instructions are
used to process four (SSE) or eight (AVX) updates in parallel. We use
partitions as on GPU but we use only one level {\em i.e.} we do not partition
the lattice into blocks. However not all scalar \emph{x86} instructions have
vector counterparts. In particular direct \emph{XMM} registers gather and
scatter and vectorized integer operations for full length AVX 256-bit registers
are missing, which makes impossible to port random number generator from
128-bit SSE to 256-bit AVX.


\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{Benchmarks}

Our CPU \emph{OpenMP} and \emph{SSE/AVX} implementation compiled via GCC 4.4 or
higher and running on quad core \emph{Intel Core i5 2.5 Ghz} CPU presented
$15\times$ performance boost comparing to single threaded scalar code. This
gives $\sim15$ Gflops that is $\sim10\%$ of the $160$ Gflops peak performance
of tested i5 CPU for 3D $64^3$ lattice case. There is no significant difference
switching from SSE to AVX. \\

\innerblock{Performance Boost}{
\begin{center}
\def\arraystretch{1.1}%
\setlength{\tabcolsep}{1em}
\huge
\begin{tabular}{ r r r | r }
  Lattice  &     GPU      &      CPU     & Boost \\
  \hline
   $32^3$  &  $1.262 ns$  &  $4.316 ns$  &  $3.419 \times$ \\
   $64^3$  &  $0.165 ns$  &  $4.857 ns$  & $29.436 \times$ \\
  $128^3$  &  $0.125 ns$  &  $5.444 ns$  & $43.552 \times$ \\
  $256^3$  &  $0.113 ns$  &  $7.099 ns$  & $62.823 \times$
\end{tabular}
\end{center}
}

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{Conclusions}

Our benchmarks give visible advantage to GPU, which is less than promised by
many publications, however much higher that comes from theoretical performance
comparison of tested \emph{i5 CPU 2.5 Ghz} to \emph{GTX 470}. This can be
traced back to 128-bit only \emph{Tausworthe} RNG implementation, inefficient
gather/scatter operations and smaller CPU memory bandwidth. \\

\emph{Intel x86} gather/scatter instructions initially planned for \emph{Sandy
Bridge} AVX standard, were postponed to \emph{Haswell} AVX2 planned for 2013.
As soon AVX2 capable CPU devices appear on the market we plan to revise our
evaluation. \\

We also consider evaluating \emph{NVIDIA Kepler} devices to determine if their
improved global memory cache can replace complicated shared memory access
scheme.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{References}

\renewcommand{\section}[2]{}
\begin{thebibliography}{9}
\bibitem{parisi} G.~Parisi ``Statistical Field Theory''
  Chapter 5, Perseus Books Publishing (1998).
\bibitem{weigel} M.~Weigel, J. Comput. Phys. \textbf{231}, 3064 (2012).
\bibitem{howes_thomas07}
Lee Howes and David Thomas.
``{\em Efficient random number generation and application using {CUDA}.}''
In Hubert Nguyen, editor, {\em GPU Gems 3}, chapter~37. Addison
  Wesley, August 2007.
\end{thebibliography}

\end{blocknode}

\end{tikzpicture}

\end{document}
