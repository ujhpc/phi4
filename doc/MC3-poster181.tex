%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Poster [181] for Facing the Multi-Core Challenge conference Stuttgart 2012
% http://www.multicore-challenge.org
% 
% Authors: P. Bialas, J. Kowal, A. Strzelecki
% http://www.uj.edu.pl/web/zaklad-technologii-gier
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{a0poster}

\newcommand{\vphi}{\vec{\varphi}}
\newcommand{\vi}{{\vec{i}}}
\newcommand{\vj}{{\vec{j}}}
\newcommand{\vmu}{\vec{\mu}}

\usepackage{tikzposter}

\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows, shapes, backgrounds}
\usetikzlibrary{shadows}

\renewcommand{\margin}{2}
% \renewcommand{\blockspacing}{2}
% \renewcommand{\blocktitlehight}{3}
% \renewcommand{\colnumber}{3}
% \renewcommand{\authorshift}{10}

\definecolor{colorone}{HTML}{006633} % default 116699
\definecolor{colortwo}{HTML}{DDFF99} % default CCCCCC
% \definecolor{colorthree}{HTML}{991111} % default 991111

% size of the document
\usepackage[margin=\margin cm, paperwidth=84.1cm, paperheight=118.9cm]{geometry}

% changing the fonts
% \usepackage[light,math]{iwona}
% \usepackage[light]{kurier}
\usepackage{lmodern}
\renewcommand*\familydefault{\sfdefault}
% \usepackage{cmbright}
\usepackage[T1]{fontenc}

% add your packages here
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{bold-extra}
\usepackage{multirow}

\lstdefinelanguage
  [GCC]{C++}
  []{C++}
{
  morekeywords={[2]__attribute__},%
  morekeywords={[3]vector_size, vec_t, uvec_t, fvec_t}%
}

\lstset{
  language={[GCC]C++},%
  basicstyle=\small\ttfamily,%
  keywordstyle=\ttfamily\bfseries,%
  keywordstyle=[2]\bfseries\color{darkgray},%
  keywordstyle=[3]\itshape,%
  commentstyle=\color{gray},%
  stringstyle=\color{green},%
% numbers=left,%
% numberstyle=\tiny,%
  stepnumber=1,%
  numbersep=5pt,%
% backgroundcolor=\color{lightlightgray},%
% frame=single,%
% rulecolor=\color{lightgray},%
% frameshape={RYRYNYYYY}{yny}{yny}{RYRYNYYYY}, 
  tabsize=2,%
  captionpos=b,%
  breaklines=true,%
  breakatwhitespace=false,%
  showspaces=false,%
  showtabs=false,%
  columns=flexible%
}

% \titlepic{}

\title{GPU-accelerated and CPU SIMD Optimized \\
Monte Carlo Simulation of $\phi^4$ Model}

\author{Piotr Bialas,  Jakub Kowal \and Adam Strzelecki \\
Faculty of Physics, Astronomy and Applied Computer Science \\
Jagiellonian University \\
ul. Reymonta 4, 30-059 Krakow, Poland \\
\\
\texttt{www.uj.edu.pl/web/zaklad-technologii-gier}
}

% defined in tikzposter, but now we want to include logo
\renewcommand{\titleblock}[2][($0.5*(0, \paperheight)-(0, \margin)$)] {%
\node[draw, frame, color=colortwo, fill=white, anchor=north, text=black]
  (title) at #1 {%
  \begin{minipage}{1cm}
    \includegraphics{./UJ-logo.pdf}
  \end{minipage}
  \begin{minipage}{#2 cm}
    \maketitle
  \end{minipage}
  };
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} \AddToShipoutPicture{\BackgroundPicture}

\noindent % to have the picture right in the center
\begin{tikzpicture}
\initializesizeandshifts
\titleblock{50}

\begin{blocknode}[($(title.south)-(xshift)-(yshift)$)]{Introduction}

This contribution is concerned with an efficient implementation of the
Monte-Carlo simulations of the $\varphi^4$ model\cite{parisi}. The problem is
defined as follows: having a vector field $\vphi$ defined on a regular
rectangular two or three dimensional grid we want to generate the field
configurations with probability proportional to $\exp(-H(\vphi))$ where
\begin{equation}\label{eq:ham}\begin{split}
H(\varphi)&=\sum_{\vi}\Biggl(
\frac{1}{2}\sum_{\mu=1}^d(\vphi_{\vi+\hat{\mu}}-\vphi_{\vi})^2
+\frac{\mu^2}{2}|\vphi_{\vi}|^2+
\frac{g}{24}(|\vphi_{\vi}|^2)^2\\
&\phantom{=\sum_{\vi}\bigl(}+
\frac{1}{2\Lambda}\Bigl(\sum_{\mu=1}^d(\vphi_{\vi+\hat{\mu}}-2\vphi_{\vi}+\vphi_{\vi-\vmu})\Bigr)^2\Biggr).
\end{split}
\end{equation}
In the above expression $\vi$ denotes the grid point,
$\vphi_\vi$ denotes the $N$ component vector at the grid point $\vi$
 and $\vmu$ denotes the unit displacement on the grid in direction $\mu$.

The actual  generation is done by the mean of the Metropolis  algorithm as follows: for a given lattice point $\vj$ we produce a new field
\begin{equation}
\widetilde{\vphi}_\vj=\vphi_\vj+\vec{\epsilon}
\end{equation}
where $\epsilon$ is some random vector. The we calculate the difference
$\Delta H = H(\widetilde{\vphi})-H(\vphi)$.
We then replace $\vphi_\vj$ with $\widetilde{\vphi}_\vj$ with the probability
\begin{equation}
P=\max\left\{1,\exp(-\Delta H)\right\}.
\end{equation}
The crucial feature of this algorithm is that the $\Delta H$ depends
only on the immediate neighborhood of the point $\vj$. In our case
because of the last term in (1) this neighborhood is
extended compared to usual nearest neighbors (see Figure~2 ).

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{Parallelization}

While model is inherently parallelizable, grid points that lie in the same
neighborhood cannot be updated together. Taking into account a larger
neighborhood means that a simple checkerboard decomposition pattern cannot be
used and we have devised a new grid decomposition scheme. 

\def\bksize{16}
\def\bkcount{4}

\begin{center}

\begin{tikzpicture}[scale=.255*1.25]

\pgfmathtruncatemacro\lcsize{\bksize * \bkcount}
\pgfmathtruncatemacro\lcsizem{\lcsize - 1}
\pgfmathtruncatemacro\lcsizeb{\lcsize - \bksize}
\pgfmathtruncatemacro\bksizea{\bksize - 2}
\pgfmathtruncatemacro\bksizeb{\bksize - 1}
\pgfmathtruncatemacro\bksizec{\bksize * 2}
\pgfmathtruncatemacro\bksizem{\bksizec - 1}
\pgfmathtruncatemacro\bksized{\bksizec + 1}

\draw[very thin, gray] (-.5, -.5) grid (\lcsize - .5, \lcsize - .5);

\foreach \x in {0, \bksize, ..., \lcsizeb}
  \foreach \y in {0, \bksize, ..., \lcsizeb} {
    \pgfmathtruncatemacro\n{Mod(\x/\bksize, 2)+2 * Mod(\y/\bksize, 2)+1}
    \draw[colorone, xshift=\x cm, yshift=\y cm, rounded corners=2pt]
      (-.4, -.4) rectangle
      node[red, fill opacity=.5, anchor=center, font=\Huge\bfseries\sffamily]
      {\n} (\bksize - .6, \bksize - .6);
  }

\foreach \x in {0, ..., \lcsizem}
  \foreach \y in {0, ..., \lcsizem}
    \path[draw=black, fill=white] (\x, \y) circle(.25);

\foreach \x in {\bksize, ..., \bksizem}
  \foreach \y in {\bksize, ..., \bksizem}
    \fill[black] (\x, \y) circle(.25);

\foreach \x in {\bksize, ..., \bksizem}
  \foreach \y in {\bksizea, \bksizeb, \bksizec, \bksized} {
    \fill[gray] (\x, \y) circle(.25);
    \fill[gray] (\y, \x) circle(.25);
  }

\foreach \p in {(\bksizeb, \bksizeb), (\bksizeb, \bksizec),
                (\bksizec, \bksizeb), (\bksizec, \bksizec)}
  \fill[gray] \p circle(.25);

\end{tikzpicture}

\end{center}

\textbf{Figure 1} The partition of the lattice into blocks. The blocks
with same number are processed in parallel by different thread blocks.

\begin{center}

\begin{tikzpicture}[scale=.905*1.25]

\clip[rounded corners=0cm] (-1, -1) rectangle (\bksize + 1, \bksize + 1);

\draw[help lines] (-1, -1) grid (\bksize + 1, \bksize + 1);

% lattice points
\foreach \x in {0, ..., \bksize}
  \foreach \y in {0, ..., \bksize}
    \draw (\x, \y) circle (.16);

% neighborhood
\def\neighborhood{--
  ++( 0,  -.5)-- ++( 1,  0)-- ++( 0, -.5 )-- ++(0, -.5)-- ++( 1, 0  )--
  ++( 0, -1.0)-- ++( 1,  0)-- ++( 0,  1.0)-- ++(1, 0  )-- ++( 0, 1.0)--
  ++( 1,  0  )-- ++( 0,  1)-- ++(-1,  0  )-- ++(0, 1  )-- ++(-1, 0  )--
  ++( 0,  1  )-- ++(-1,  0)-- ++( 0, -1  )--
  ++(-1,  0  )-- ++( 0, -1)-- ++(-1,  0  )-- ++(0, -.6)}

\pgfmathtruncatemacro\lchalf{\bksize / 2}

\foreach \p in {0, 1} % pass 1 - neighbourhoods, 2 - points
  \foreach \x in {0, ..., \bksize}
    \foreach \y in {0, ..., \bksize} {
      \pgfmathtruncatemacro\my{Mod(\y, 2)}
      \ifnum \my = 0
        \pgfmathtruncatemacro\ny{Mod(\y, 4)}
        \ifnum \ny = 2
          \pgfmathtruncatemacro\mx{Mod(\x+2, 4)}
        \else
          \pgfmathtruncatemacro\mx{Mod(\x, 4)}
        \fi
        \ifnum \mx = 0
          \ifnum \p = 0
            \def\c{0}
            \ifnum \x = \lchalf
              \ifnum \y = \lchalf
                \def\c{1}
              \fi
            \fi
            \ifnum \c = 0
              \path[draw=black, fill=black, fill opacity=.2,
                    rounded corners=.25cm, shorten >=2pt]
                (\x - 2.5, \y) \neighborhood;
            \fi
          \else
            \fill[red] (\x, \y) circle(.16);
          \fi
        \fi
      \fi
    }

% main center neighborhood
\path[draw=red, fill=red, fill opacity=.4, rounded corners=.25cm, shorten >=2pt, 
      line width=.75mm, cap=round, join=round]
      (\lchalf - 2.5, \lchalf) \neighborhood;

\end{tikzpicture}

\end{center}

\textbf{Figure 2} Partitioning of the blocks into disjoint sublattices.
Area marked in re denotes  neighborhood used in updating the center point.
Read points are processes in parallel by different threads of the same thread
block.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{References}

\renewcommand{\section}[2]{}
\begin{thebibliography}{9}
\bibitem{parisi} G.~Parisi ``Statistical Field Theory''
  Chapter 5, Perseus Books Publishing (1998).
\bibitem{weigel} M.~Weigel, J. Comput. Phys. \textbf{231}, 3064 (2012).
% \bibitem{howes_thomas07}
% Lee Howes and David Thomas.
% ``{\em Efficient random number generation and application using {CUDA}.}''
% In Hubert Nguyen, editor, {\em GPU Gems 3}, chapter~37. Addison
%   Wesley, August 2007.
\end{thebibliography}

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{blocknode}{Tausworthe RNG}

% \lstinputlisting{MC3-tausworthe.cpp}

% \end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}[($(title.south)+(xshift)-(yshift)$)]{GPU Implementation}

  On GPU we adopt the hierarchical scheme from ref.~\cite{weigel}
  suitably modified to account for bigger neighborhood. We first
  divide the whole lattice in blocks of $32\times 32$ points in 2D or
  $16\times16\times16$ in 3D (see Figure~1).  Then we start a kernel
  that process every fourth block. Each block is assigned to a block
  of 128 threads or 256 in 3D. 
  First we fetch the values of the fields from global
  to shared memory (including border points).

  After that each thread updates one point from the first partition
  (see figure ...).  Each point is updated few, typically 4-8 times in
  a row.  That speeds up the computations as most of the precalculated
  needed values is already in the registers. This so called multi-hit
  Metropolis algorithm.


  Then after synchronization, next partition is updated and so on.
  After processing all eight(2D) or 16 (3D) partitions, we repeat
  the whole process up to 50 times, using the values already in shared
  memory. Together with multi--hit  algorithm described above this
  reduces drastically the time taken by global memory loads. 

  Finally the kernel writes the shared memory back into global memory and
  new kernel is started processing next batch of blocks.

  \subsubsection*{Performance}

  The performance of the algorithm increases with growing lattice
  size. For smaller lattices there is not enough thread blocks
  launched to use the GPU resources efficiently.  Altogether in this
  way we managed to achieve $0.11$ nanoseconds for single field update
  on a $256^3$ 3D lattice using \emph{NVIDIA GTX 470} GPU, reaching
  around $517$ Gflops that is $47\%$ of $1088$ Gflops peak performance
  of this device.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}{CPU Implementation}

  For CPU vs GPU comparison we provide multithreaded vectorized CPU
  implementation. It uses OpenMP for parallel execution, SSE/AVX and
  compiler vector extensions for vectorization. This implementation
  does mimic GPU SIMT execution model. The SIMD instructions are used
  to process four (SSE) or eight (AVX) updates in parallel. After
  programming using the SSE intrinsics we switched to using the vector
  types provided in most of the modern compilers. This yields
  identical assembly code and is easier to write and maintain.

  However not all scalar \emph{x86} instructions have vector
  counterparts. In particular direct \emph{XMM} registers gather and
  scatter and vectorized integer operations for full length AVX
  256-bit registers are missing, which makes impossible to port random
  number generator from 128-bit SSE to 256-bit AVX. 


  We use
  partitions as on GPU but we use only one level {\em i.e.} we do not partition
  the lattice into blocks. 

\subsubsection*{Performance}

Our CPU \emph{OpenMP} and \emph{SSE/AVX} implementation compiled via GCC 4.4 or
higher and running on quad core \emph{Intel Core i5 2.5 Ghz} CPU presented
$15\times$ performance boost comparing to single threaded scalar code. This
gives $\sim15$ Gflops that is $\sim10\%$ of the $160$ Gflops peak performance
of tested i5 CPU for 3D $64^3$ lattice case. There is no significant difference
switching from SSE to AVX.

Contrary to the implementation on the GPU the performance of the CPU
implementation decreases with the lattice size. This is due to an
inefficient memory access pattern and many resulting cache
misses. This can be alleviated or eliminated by adopting the second
level of partitioning the lattice as on the GPU. This is the subject of
an ongoing work.

Another performance bottleneck are the gather/scatter vector
operations.  As the values accessed by vector SSE instructions are not
in consecutive memory addresses they have to be downloaded separately
and then assembled together into one vector word. Same process applies
to writing back. A possible solution would be rearranging the layout so
that the values used by vector operations do lie at consecutive
addresses in the memory.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{colorone}{HTML}{FFFFFF}
\definecolor{colortwo}{HTML}{991111}

\blocknodew[($(box.south)-(yshift)-(9.25,0)$)]{20.5}{Benchmark}{
\begin{center}
\def\arraystretch{1.1}%
\setlength{\tabcolsep}{1em}
\large
\begin{tabular}{ r r r | r }
    Size   & GPU\footnotemark & CPU\footnotemark & Boost \\
  \hline
   $32^3$  &  $1.262 ns$  &  $4.316 ns$  &  $3.419 \times$ \\
   $64^3$  &  $0.165 ns$  &  $4.857 ns$  & $29.436 \times$ \\
  $128^3$  &  $0.125 ns$  &  $5.444 ns$  & $43.552 \times$ \\
  $256^3$  &  $0.113 ns$  &  $7.099 ns$  & $62.823 \times$
\end{tabular}
\footnotetext{$^1$\emph{GeForce GTX 470} 1.67 Ghz, 1.25GB, 448 CUDA cores, 2.0 capability}
\footnotetext{$^2$\emph{Intel Core i5-2400S CPU} 2.50 GHz, 4 core, 6MB Cache, SSE4.x/AVX}
\end{center}

}

\blocknodew[($(box.north)+(20.25,0)$)]{16.5}{Breakdown}{
\begin{center}
\def\arraystretch{1.1}%
\setlength{\tabcolsep}{1em}
\large
\begin{tabular}{ r r r }
  Type      &     GPU      &  CPU \\
  \hline
  gather    &  $14.19 \%$  &  \multirow{2}{*}{$59.73 \%$} \\
  corona    &  $22.84 \%$  &  \\
  RNG       &  $ 6.66 \%$  &  $13.88 \%$ \\
  exp/log   &  $23.86 \%$  &  $ 6.22 \%$ \\
  update    &  $25.75 \%$  &  $20.17 \%$
\end{tabular}
\end{center}
}

\definecolor{colorone}{HTML}{006633}
\definecolor{colortwo}{HTML}{DDFF99}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocknode}[($(box.south)-(11,.25)-(yshift)$)]{Conclusions}

Our benchmarks give visible advantage to GPU, which is less than promised by
many publications, however much higher that comes from theoretical performance
comparison of tested \emph{i5 CPU 2.5 Ghz} to \emph{GTX 470}. This can be
traced back to 128-bit only \emph{Tausworthe} RNG implementation, but mostly to
the inefficient memory access pattern and vector gather/scatter operations.

\emph{Intel x86} gather/scatter instructions initially planned for \emph{Sandy
Bridge} AVX standard, were postponed to \emph{Haswell} AVX2 planned for 2013.
As soon AVX2 capable CPU devices appear on the market we plan to revise our
evaluation. 
We also consider evaluating \emph{NVIDIA Kepler} devices to determine if their
improved global memory cache can replace complicated shared memory access
scheme.

\end{blocknode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{tikzpicture}

\end{document}
