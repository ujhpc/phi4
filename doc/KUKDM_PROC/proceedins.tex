\documentclass[a4paper]{llncs}
\usepackage[plain]{algorithm}
%\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows,shapes,backgrounds}
\usetikzlibrary{shadows}

%\input {header.tex}
\renewcommand{\a}[1]{\v{a}_{#1}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\v}[1]{\vec{#1}}
\newcommand{\av}[1]{\langle#1\rangle}
\newcommand{\vphi}{\vec{\varphi}}
\newcommand{\vi}{{\vec{i}}}
\newcommand{\vj}{{\vec{j}}}
\newcommand{\vmu}{\vec{\mu}}

\newcommand{\redblack}[3]{ 
\pgfmathtruncatemacro{\odd}{Mod(#1+#2,2)}
   \ifnum \odd=0
      \fill[fill=black] (#1,#2) circle[radius=#3];
   \else
      \fill[fill=red] (#1,#2) circle[radius=#3];
   \fi
}

\newcommand{\fillblockgray}[2]{
\pgfmathtruncatemacro\llx{\bksize*(#1)-2}
\pgfmathtruncatemacro\lly{\bksize*(#2)-2}
\pgfmathtruncatemacro\urx{\bksize*(#1+1)+1}
\pgfmathtruncatemacro\ury{\bksize*(#2+1)+1}
\foreach \x in {\llx, ..., \urx}
  \foreach \y in {\lly,...,\ury} {
    \fill[gray] (\x, \y) circle(.25);
  }
}

\newcommand{\npartition}[3]{
\pgfmathtruncatemacro\row{Mod(#2,4)}
\ifnum \row = 0
   \pgfmathtruncatemacro\num{Mod(#1,4)}
\else \ifnum \row  = 1
   \pgfmathtruncatemacro\num{Mod(#1,4)+4}
\else \ifnum \row  = 2
   \pgfmathtruncatemacro\num{Mod(Mod(#1,4)+2,4)}
\else \ifnum \row  = 3
   \pgfmathtruncatemacro\num{Mod(Mod(#1,4)+2,4)+4}
\fi
\fi
\fi
\fi
\ifnum \num = #3
\fill[black] (#1, #2) circle(0.35);
\fi

}

\newcommand{\markpartition}[3]{
\pgfmathtruncatemacro\llx{\bksize*(#1)}
\pgfmathtruncatemacro\lly{\bksize*(#2)}
\pgfmathtruncatemacro\urx{\bksize*(#1+1)-1}
\pgfmathtruncatemacro\ury{\bksize*(#2+1)-1}
\foreach \x in {\llx, ..., \urx}
  \foreach \y in {\lly,...,\ury} {
    \npartition{\x}{\y}{#3}
  }
}


\newcommand{\partition}[3]{
\pgfmathtruncatemacro\lchalf{\bksize / 2}
      \pgfmathtruncatemacro\my{Mod(#1, 2)}
      \ifnum \my = 0
        \pgfmathtruncatemacro\ny{Mod(#1, 4)}
        \ifnum \ny = 2
          \pgfmathtruncatemacro\mx{Mod(#2+2, 4)}
        \else
          \pgfmathtruncatemacro\mx{Mod(#2, 4)}
        \fi
        \ifnum \mx = 0
            \fill[red] (#2, #1) circle(#3);
          \fi
         \fi         
       }


\def\neighborhood{--
  ++(0,-0.5)-- ++(1,0)-- ++(0,-0.5)-- ++(0,-0.5)-- ++(1,0)--
  ++(0,-1.0)-- ++(1,0)-- ++(0,1.0)-- ++(1,0)-- ++(0,1.0)-- ++(1,0)--
  ++(0,1)-- ++(-1,0)-- ++(0,1)-- ++(-1,0)-- ++(0,1)-- ++(-1,0)-- ++(0,-1)--
  ++(-1,0)-- ++(0,-1)-- ++(-1,0)-- ++(0,-0.6)}

\definecolor{colorone}{HTML}{006633} % default 116699
\definecolor{colortwo}{HTML}{DDFF99} % default CCCCCC
\def\bksize{8}
\def\bkcount{4}
\def\lcsize{5}

\title{ GPU-accelerated and CPU SIMD Optimized \\ Monte Carlo Simulation of $\phi^4$ Model}
\author{Piotr Bialas\inst{1}\inst{2} \and Jakub Kowal\inst{1} \and Adam Strzelecki\inst{1}}
\institute{Faculty of Physics, Astronomy and Applied Computer Science\\
Jagiellonian University ul. Reymonta 4, 30-059 Krakow, Poland 
\and Mark Kac Complex Systems Research Centre\\
  Faculty of Physics, Astronomy and Applied Computer Science\\
  Jagellonian University, Reymonta 4, 30--059 Krakow, Poland}
\begin{document}


\maketitle

\section{Introduction}

\section{Ginzburg--Landau model and Monte-Carlo simulations}
This contribution is concerned with an efficient implementation of the
Monte-Carlo simulations of the $\varphi^4$ or Ginzburg--Landau model.
This model in its discretized form is defined by the hamiltonian\cite{parisi}: 
\begin{equation*}\label{eq:hamiltonian}\begin{split}
H[\varphi]&=\sum_{\vi}\Biggl(
\frac{1}{2}\sum_{\mu=1}^d(\vphi(x_\vi+\a{\mu})-\vphi(x_\vi))^2\\
&\phantom{=\sum_{\vi}\bigl(}+\frac{\mu^2}{2}|\vphi(x_{\vi})|^2+
\frac{g}{24}(|\vphi(x_{\vi})|^2)^2\\
&\phantom{=\sum_{\vi}\bigl(}+
\frac{1}{2\Lambda}\Bigl(\sum_{\mu=1}^d
(\vphi(x_\vi+\a{\mu})-2\vphi(x_{\vi})+\vphi(x_\vi-\a\mu))\Bigr)^2\Biggr).
\end{split}
\end{equation*}
The
problem is defined as follows: having a vector field $\vphi$ defined
on a regular rectangular two or three dimensional grid we want to
generate the field configurations with probability proportional to
$\exp(-H(\vphi))$.  

The actual generation is done by the mean of the Metropolis
algorithm. This amounts to sequentially updating all the points of the
lattice. During each update we change the value of the field $\vphi$
in on a single lattice point ($\vphi_i$ is a shorthand for $\vphi(x_\vi)$).
\begin{equation}
\vphi_i\longrightarrow\widetilde{\vphi_i}=\vphi_i+\eta.
\end{equation}
The $\eta$ is a pseudo--random number drawn from a distribution such
that $P(\eta) = P(-\eta)$. In practice we draw the $\eta$ from the
uniform distribution on interval $[-\varepsilon,\varepsilon)$.  The we
calculate the difference of the hamiltonians
\begin{equation}
\Delta H=H[\widetilde{\vphi_i}]-H[\vphi_i]
\end{equation}
If $\Delta H < 0$ we accept the change and substitute
$\widetilde{\vphi_i}$ for $\vphi_i$, otherwise we accept the change
with the probablity $\exp(-\Delta H)$. In other words we accept the
change with the probability
\begin{equation}
P(\vphi_i\rightarrow\vphi_i+\varepsilon)=\min\left(1,\exp(-\Delta H)\right)
\end{equation}
The crucial feature of this algorithm is that the update is local {\em
  i.e.} $\Delta H$ depends only on the values of fields in the
immediate neighborhood of the updated point 
\begin{equation}
\begin{split}
\Delta H & = -(\widetilde{\vphi_i}-\vphi_i)
\underbrace{\left(c^{01}_i-\frac{1}{\Lambda}(c^{02}_i-4 d c^{01}_i+2 c^{11}_i )\right)}_{c_i}\\ 
&\phantom{= -}
+(\widetilde{\vphi}^2_i-\vphi_i^2)\left(d+\frac{\mu^2}{2}+d (1+2d)\frac{1}{\Lambda}\right)\\
&\phantom{= -}+\frac{g}{4!}\left(\widetilde{\vphi}^4_i-\vphi^4_i\right)
\end{split}
\end{equation}
where
\begin{equation}\begin{split} 
c_i^{01}&=\sum_{\mu=1}^d\left(\vphi(x_\vi+\a\mu)+\vphi(x_\vi-\a\mu)\right)\\
c_i^{02}&=\sum_{\mu=1}^d\left(\vphi(x_\vi+2\a\mu)+\vphi(x_\vi-2\a\mu)\right)\\
c_i^{11}&=\sum_{\mu=}^d\sum_{\nu=1}^{\mu-1}
\bigl(\vphi(x_\vi+\a\mu+\a\nu)+\vphi(x_\vi-\a\mu-\a\nu)\\
&\phantom{+=\sum_{\mu=}^d\sum_{\nu=1}^{\mu-1}
}
+ \vphi(x_\vi+\a\mu-\a\nu)+\vphi(x_\vi-\a\mu+\a\nu)
\bigr)
\end{split}
\end{equation}
The term $c_i$ is refered to as {\em corona}. 
In our case, because of
the laster term in \eqref{eq:hamiltonian}, this neighborhood is
extended compared to usual nearest neighbors (see figure~\ref{fig:nn}). 


\section{GPU implementation}

While model is inherently parallelizable, grid points that lie in the
same neighborhood cannot be updated together. Taking into account a
larger neighborhood means that a simple checkerboard decomposition
pattern cannot be used and we have devised a new grid decomposition
scheme. We choose the decomposition depicted in the
figure~\ref{fig:part}.
\begin{figure}
\begin{center}
\begin{tikzpicture} 
%\pgfmathtruncatemacro\lcsize{\bksize * \bkcount}
\def\lcsize{8}
\pgfmathtruncatemacro\lcsizem{\lcsize - 1}
\pgfmathtruncatemacro\lcsizeb{\lcsize - \bksize}
\pgfmathtruncatemacro\bksizea{\bksize - 2}
\pgfmathtruncatemacro\bksizeb{\bksize - 1}
\pgfmathtruncatemacro\bksizec{\bksize * 2}
\pgfmathtruncatemacro\bksizem{\bksizec - 1}
\pgfmathtruncatemacro\bksized{\bksizec + 1}
\draw[very thin, gray] (-.5, -.5) grid (\bksize - .5, \bksize - .5);
\foreach \x in {0,...,\lcsizem} 
\foreach \y in {0, ...,\lcsizem}
\draw[fill=white,draw=black]  (\x, \y) circle[radius=0.1];

\coordinate (c) at (4,4);

\node at (c)  [circle,fill=red,draw=black] {};

\def\coronaa{(0,1), (0,-1), (1,0), (-1,0)}
\def\coronab{(0,2), (0,-2), (2,0), (-2,0)}
\def\coronac{(1,1), (1,-1), (-1,-1), (-1,1)}

\foreach \p in \coronaa {
\path (c) +\p node  [rectangle,fill=gray, draw=black]    {};
}

\foreach \p in \coronab {
\path (c) +\p node  [diamond,fill=gray,draw=black]    {};
}

\foreach \p in \coronac {
\path (c) +\p node  [circle,fill=gray, draw= black]    {};
}

\end{tikzpicture}
\end{center}
\caption{\label{fig:nn}The neigborhood used to calculate the $\Delta H$:
$c^{01}$ -- squares, $c^{11}$ -- circles and $c^{02}$ -- diamonds.}
\end{figure}

\begin{figure}
\begin{center} 
\begin{tikzpicture}[scale=0.95]

% \clip[rounded corners=0cm] (-1, -1) rectangle (\bksize + 1, \bksize + 1);

% \draw[help lines] (-1, -1) grid (\bksize + 1, \bksize + 1);

\pgfmathtruncatemacro\bksizem{\bksize - 1}

% % lattice points
 \foreach \x in {0, ..., \bksizem}
   \foreach \y in {0, ..., \bksizem} {
     \pgfmathtruncatemacro\four{Mod(\y,4)}
     \pgfmathtruncatemacro\eight{Mod(\x,4)+4*Mod(\y,2)}
     \ifnum \four<2
     \pgfmathtruncatemacro\eight{Mod(\x,4)+4*Mod(\y,2)}
     \node[draw,rectangle,minimum size=0.6cm] at (\x,\y)  {\eight};
     \else
     \pgfmathtruncatemacro\eight{Mod(\x+2,4)+4*Mod(\y,2)}
     \node[draw,rectangle,minimum size=0.6cm] at (\x,\y)  {\eight};
     \fi
}
% neighborhood
\def\neighborhood{--
  ++( 0,  -.5)-- ++( 1,  0)-- ++( 0, -.5 )-- ++(0, -.5)-- ++( 1, 0  )--
  ++( 0, -1.0)-- ++( 1,  0)-- ++( 0,  1.0)-- ++(1, 0  )-- ++( 0, 1.0)--
  ++( 1,  0  )-- ++( 0,  1)-- ++(-1,  0  )-- ++(0, 1  )-- ++(-1, 0  )--
  ++( 0,  1  )-- ++(-1,  0)-- ++( 0, -1  )--
  ++(-1,  0  )-- ++( 0, -1)-- ++(-1,  0  )-- ++(0, -.6)}

\pgfmathtruncatemacro\lchalf{\bksize / 2}


% main center neighborhood
\path[draw=red,  rounded corners=.1cm, shorten >=2pt, line width=.75mm]
(\lchalf - 2.5, \lchalf) \neighborhood;
\end{tikzpicture}
\end{center}
\caption{\label{fig:part}Partition of the lattice into independent
  sublattices. Squares with same number can be updated in parallel.}
\end{figure}
In 2D we need eight sublattices and 16 sublattices in 3D. 

% GPU implementation


The GPU is no exception as to what concerns the speed of the memeory
access.  While very fast the throughput from global memory is still
almost two orders of magnitude slower then needed to sustain the
maximal flops output.  The NVidia card we use contains 14 {\em Scalar
  Multiprocessors} (SM). Each SM is equiped with up 48KB shared memory
and 32K of 32bit registers\cite{Fermi} which act as user managed
cache. While starting from the compute capability 2.0 NVidia cards are
also equiped with L1 and L2 caches the use of automatic cache is, as
we will show later, no advantageous in terms of performance.

To efficiently use the shared memory  we adopt the hierarchical scheme from
ref.~\cite{weigel} suitably modified to account for bigger
neighborhood.  We first divide the whole lattice in blocks of
$B_x\times B_y$ points (see figure~\ref{fig:blocks}). 
\begin{figure} 
\begin{center}

\begin{tikzpicture}[scale=0.35]

\pgfmathtruncatemacro\lcsize{\bksize * \bkcount}
\pgfmathtruncatemacro\lcsizem{\lcsize - 1}
\pgfmathtruncatemacro\lcsizeb{\lcsize - \bksize}


\draw[very thin, gray] (-.5, -.5) grid (\lcsize - .5, \lcsize - .5);

\foreach \x in {0, \bksize, ..., \lcsizeb}
  \foreach \y in {0, \bksize, ..., \lcsizeb} {
    \pgfmathtruncatemacro\n{Mod(\x/\bksize, 2)+2 * Mod(\y/\bksize, 2)+1}
    \draw[colorone, xshift=\x cm, yshift=\y cm, rounded corners=2pt]
      (-.4, -.4) rectangle
      node[red, fill opacity=0, anchor=center, font=\Huge\bfseries\sffamily]
      {\n} (\bksize - .6, \bksize - .6);
  }

\foreach \x in {0, ..., \lcsizem}
  \foreach \y in {0, ..., \lcsizem}{
 
    \path[draw=black, fill=white] (\x, \y) circle(.25);
}


\fillblockgray{0}{0}
\markpartition{0}{0}{0}
\fillblockgray{0}{2}
\markpartition{0}{2}{0}
\fillblockgray{2}{0}
\markpartition{2}{0}{0}
\fillblockgray{2}{2}
\markpartition{2}{2}{0}
\end{tikzpicture}
\end{center}
\caption{\label{fig:blocks}Partition of the lattice into
  blocks. Blocks with same number can be updated in parallel. Each
  block is loaded first into shared memory.}
\end{figure}
To process each point inside a block we also need the neighboring
points.  We divide the grid of blocks into subgrids in such a way,
that blocks together with the neigborhood in each subgrid do not
overlap (see gray points in figure~\ref{fig:blocks}). In 2D we need
four of such subgrids and eight in 3D case.

We then start a kernel that process all blocks in one subgrid.  
Each block is assigned to a block of
$N_{th}$ threads (CTA -- Cooperative Threads Array). 
First we fetch the values of the fields from global to
shared memory (including border points).  
After that each thread
updates one point from the first sublattice. Then after
synchronization, next sublattice is updated and so on. After processing
all eight(2D) or 16 (3D) sublattices the kernel writes the shared
memory back into global and new kernel is started processing next
subgrid  of blocks.  

Altogether in this way we managed to achieve $0.13$
nanoseconds for single lattice field update on \emph{NVIDIA GTX 470},
reaching around $430$ Gflops that is $40\%$ of $1088$ Gflops peak
performance of this device.  We use the Tausworthe
RNG\cite{howes_thomas07}.
\begin{algorithm}
\begin{algorithmic}[1]
\For{every global sweep}
\For{every block subgrid}
\For{every block in subgrid}
\State load block  into shared memory
\For{every local sublattice}
\For{every site $i$ in local sublattice}
\State calculate corona $c_i$ 
\State update  site  $N_hit$ times
\EndFor 
\State {\tt \_\_syncthreads();}
\EndFor
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\caption{\label{alg:gpu}}
\end{algorithm}

\section{CPU implementation}
% CPU implementation
In order to provide unbiased CPU vs GPU speed up results we provide
multithreaded vectorized CPU implementation. It uses OpenMP for
parallel execution,  SSE/AVX and compiler vector extensions for
vectorization.  This implementation does mimic GPU SIMT execution
model.  The SIMD instructions are used to process four (SSE) or eight
(AVX) updates in parallel. We use partitions as on GPU but we use only
one level {\em i.e.} we do not partition the lattice into blocks.
However not all scalar \emph{x86} instructions have vector
counterparts.  In particular direct \emph{XMM} registers gather and
scatter and vectorized integer operations for full length AVX 256-bit
registers are missing, which makes impossible to port random number
generator from 128-bit SSE to 256-bit AVX. Initially planned for AVX
standard, these were postponed to AVX2 planned for 2013. As soon AVX2
capable CPU devices appear on the market we plan to revise our
evaluation.

\section{Performance}

\begin{table}
\begin{center}
\begin{tabular}{|r|rr|rr|}
\hline\hline
\multicolumn{1}{|}{} & \multicolumn{2}{c|}{GPU} & \multicolumn{2}{c|}{CPU}\\\hline
\multicolumn{1}{|c}{size} & \multicolumn{1}{|c|}{ns} & \multicolumn{1}{c|}{Gflops} & \multicolumn{1}{|c}{ns} & \multicolumn{1}{|c|}{Gflops}\\\hline\hline
$32^3$ & & & & \\\hline
$64^3$ & & & & \\\hline\hline
\end{tabular}
\end{center}
\caption{\label{tab:comp} Performance}
\end{table}

\begin{equation}
  t=a+ V \cdot b +
N_{glob.} V \left(c + N_{loc.}\left(d + N_{hit}\cdot e\right)\right) 
\end{equation}

\section{Discussion}

Our CPU \emph{OpenMP} and \emph{SSE/AVX} implementation compiled via
GCC 4.4 or higher and running on \emph{Intel Core i5 2.5 Ghz} quad
core CPU presented $15\times$ performance boost comparing to single
threaded scalar code and $3.76$ nanoseconds for single lattice field
update.  Which gives the $15$ Gflops that is $\sim10\%$ of the  $160$
Gflops peak performance of tested i5 CPU. There is no significant
increase in performance while switching from SSE to AVX instructions.

This gives around $28\times$ advantage to GPU, which is noticeably
less than promised by many publications, however much higher that
comes from comparison of  tested i5
CPU to \emph{GTX 470}. This can be
traced back to 128-bit only \emph{Tausworthe} random number generator
implementation and inefficient vector store and load operations
(gather/scatter).

\begin{thebibliography}{9}
\bibitem{parisi} G.~Parisi ``Statistical Field Theory'' Chapter 5, Perseus Books Publishing (1998).
\bibitem{Fermi} Fermi architecture whitepaper. 
\bibitem{weigel} M.~Weigel, J. Comput. Phys. \textbf{231}, 3064 (2012).
\bibitem{howes_thomas07}
Lee Howes and David Thomas.
``{\em Efficient random number generation and application using {CUDA}.}''
In Hubert Nguyen, editor, {\em GPU Gems 3}, chapter~37. Addison
  Wesley, August 2007.
\end{thebibliography}

\end{document}
