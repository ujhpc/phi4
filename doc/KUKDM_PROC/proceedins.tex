\documentclass[a4paper]{llncs}
\usepackage{amsmath}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows,shapes,backgrounds}
\usetikzlibrary{shadows}

%\input {header.tex}
\renewcommand{\a}[1]{\v{a}_{#1}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\v}[1]{\vec{#1}}
\newcommand{\av}[1]{\langle#1\rangle}
\newcommand{\vphi}{\vec{\varphi}}
\newcommand{\vi}{{\vec{i}}}
\newcommand{\vj}{{\vec{j}}}
\newcommand{\vmu}{\vec{\mu}}

\newcommand{\redblack}[3]{ 
\pgfmathtruncatemacro{\odd}{Mod(#1+#2,2)}
   \ifnum \odd=0
      \fill[fill=black] (#1,#2) circle[radius=#3];
   \else
      \fill[fill=red] (#1,#2) circle[radius=#3];
   \fi
}

\newcommand{\fillblockgray}[2]{
\pgfmathtruncatemacro\llx{\bksize*(#1)-2}
\pgfmathtruncatemacro\lly{\bksize*(#2)-2}
\pgfmathtruncatemacro\urx{\bksize*(#1+1)+1}
\pgfmathtruncatemacro\ury{\bksize*(#2+1)+1}
\foreach \x in {\llx, ..., \urx}
  \foreach \y in {\lly,...,\ury} {
    \fill[gray] (\x, \y) circle(.25);
  }
}

\newcommand{\npartition}[3]{
\pgfmathtruncatemacro\row{Mod(#2,4)}
\ifnum \row = 0
   \pgfmathtruncatemacro\num{Mod(#1,4)}
\else \ifnum \row  = 1
   \pgfmathtruncatemacro\num{Mod(#1,4)+4}
\else \ifnum \row  = 2
   \pgfmathtruncatemacro\num{Mod(Mod(#1,4)+2,4)}
\else \ifnum \row  = 3
   \pgfmathtruncatemacro\num{Mod(Mod(#1,4)+2,4)+4}
\fi
\fi
\fi
\fi
\ifnum \num = #3
\fill[black] (#1, #2) circle(0.35);
\fi

}

\newcommand{\markpartition}[3]{
\pgfmathtruncatemacro\llx{\bksize*(#1)}
\pgfmathtruncatemacro\lly{\bksize*(#2)}
\pgfmathtruncatemacro\urx{\bksize*(#1+1)-1}
\pgfmathtruncatemacro\ury{\bksize*(#2+1)-1}
\foreach \x in {\llx, ..., \urx}
  \foreach \y in {\lly,...,\ury} {
    \npartition{\x}{\y}{#3}
  }
}


\newcommand{\partition}[3]{
\pgfmathtruncatemacro\lchalf{\bksize / 2}
      \pgfmathtruncatemacro\my{Mod(#1, 2)}
      \ifnum \my = 0
        \pgfmathtruncatemacro\ny{Mod(#1, 4)}
        \ifnum \ny = 2
          \pgfmathtruncatemacro\mx{Mod(#2+2, 4)}
        \else
          \pgfmathtruncatemacro\mx{Mod(#2, 4)}
        \fi
        \ifnum \mx = 0
            \fill[red] (#2, #1) circle(#3);
          \fi
         \fi         
       }


\def\neighborhood{--
  ++(0,-0.5)-- ++(1,0)-- ++(0,-0.5)-- ++(0,-0.5)-- ++(1,0)--
  ++(0,-1.0)-- ++(1,0)-- ++(0,1.0)-- ++(1,0)-- ++(0,1.0)-- ++(1,0)--
  ++(0,1)-- ++(-1,0)-- ++(0,1)-- ++(-1,0)-- ++(0,1)-- ++(-1,0)-- ++(0,-1)--
  ++(-1,0)-- ++(0,-1)-- ++(-1,0)-- ++(0,-0.6)}

\definecolor{colorone}{HTML}{006633} % default 116699
\definecolor{colortwo}{HTML}{DDFF99} % default CCCCCC
\def\bksize{8}
\def\bkcount{4}
\def\lcsize{5}

\title{ GPU-accelerated and CPU SIMD Optimized \\ Monte Carlo Simulation of $\phi^4$ Model}
\author{Piotr Bialas \and Jakub Kowal \and Adam Strzelecki}
\institute{Faculty of Physics, Astronomy and Applied Computer Science\\
Jagiellonian University\\
ul. Reymonta 4, 30-059 Krakow, Poland }
\begin{document}


\maketitle


This contribution is concerned with an efficient implementation of the
Monte-Carlo simulations of the $\varphi^4$ model.
This model in its discretized form is defined by the hamiltonian\cite{parisi}: 
\begin{equation*}\label{eq:ham}\begin{split}
H[\varphi]&=\sum_{\vi}\Biggl(
\frac{1}{2}\sum_{\mu=1}^d(\vphi(x_\vi+\a{\mu})-\vphi(x_\vi))^2\\
&\phantom{=\sum_{\vi}\bigl(}+\frac{\mu^2}{2}|\vphi(x_{\vi})|^2+
\frac{g}{24}(|\vphi(x_{\vi})|^2)^2\\
&\phantom{=\sum_{\vi}\bigl(}+
\frac{1}{2\Lambda}\Bigl(\sum_{\mu=1}^d
(\vphi(x_\vi+\a{\mu})-2\vphi(x_{\vi})+\vphi(x_\vi-\a\mu))\Bigr)^2\Biggr).
\end{split}
\end{equation*}
The
problem is defined as follows: having a vector field $\vphi$ defined
on a regular rectangular two or three dimensional grid we want to
generate the field configurations with probability proportional to
$\exp(-H(\vphi))$.  

The actual generation is done by the mean of the Metropolis
algorithm. This amounts to sequentially updating all the points of the
lattice. The crucial feature of this algorithm is that the update is
local {\em i.e.} the new value of the field in a given point depends
only on the values of fields in the immediate neighborhood of the
updated point. In our case this neighborhood is extended compared to
usual nearest neighbors (see figure~\ref{fig:nn} (Right)). The update
is random and requires a good source of pseudo-random numbers. We use
the Tausworthe RNG\cite{howes_thomas07}.



While model is inherently parallelizable, grid points that lie in
the same neighborhood cannot be updated together. Taking into
account a larger neighborhood means that a simple checkerboard
decomposition pattern cannot be used and we have devised a new grid
decomposition scheme.

\begin{figure}
\begin{center}
\begin{tikzpicture} 
%\pgfmathtruncatemacro\lcsize{\bksize * \bkcount}
\def\lcsize{8}
\pgfmathtruncatemacro\lcsizem{\lcsize - 1}
\pgfmathtruncatemacro\lcsizeb{\lcsize - \bksize}
\pgfmathtruncatemacro\bksizea{\bksize - 2}
\pgfmathtruncatemacro\bksizeb{\bksize - 1}
\pgfmathtruncatemacro\bksizec{\bksize * 2}
\pgfmathtruncatemacro\bksizem{\bksizec - 1}
\pgfmathtruncatemacro\bksized{\bksizec + 1}
\draw[very thin, gray] (-.5, -.5) grid (\bksize - .5, \bksize - .5);
\foreach \x in {0,...,\lcsizem} 
\foreach \y in {0, ...,\lcsizem}
\draw[fill=white,draw=black]  (\x, \y) circle[radius=0.1];

\coordinate (c) at (4,4);

\node at (c)  [circle,fill=red,draw=black] {};

\def\coronaa{(0,1), (0,-1), (1,0), (-1,0)}
\def\coronab{(0,2), (0,-2), (2,0), (-2,0)}
\def\coronac{(1,1), (1,-1), (-1,-1), (-1,1)}

\foreach \p in \coronaa {
\path (c) +\p node  [circle,fill=gray]    {};
}

\foreach \p in \coronab {
\path (c) +\p node  [circle,fill=gray]    {};
}

\foreach \p in \coronac {
\path (c) +\p node  [circle,fill=gray]    {};
}

\end{tikzpicture}
\end{center}
\caption{}
\end{figure}

\begin{figure}
\begin{center} 
\begin{tikzpicture}[scale=0.95]

% \clip[rounded corners=0cm] (-1, -1) rectangle (\bksize + 1, \bksize + 1);

% \draw[help lines] (-1, -1) grid (\bksize + 1, \bksize + 1);

\pgfmathtruncatemacro\bksizem{\bksize - 1}

% % lattice points
 \foreach \x in {0, ..., \bksizem}
   \foreach \y in {0, ..., \bksizem} {
     \pgfmathtruncatemacro\four{Mod(\y,4)}
     \pgfmathtruncatemacro\eight{Mod(\x,4)+4*Mod(\y,2)}
     \ifnum \four<2
     \pgfmathtruncatemacro\eight{Mod(\x,4)+4*Mod(\y,2)}
     \node[draw,rectangle,minimum size=0.6cm] at (\x,\y)  {\eight};
     \else
     \pgfmathtruncatemacro\eight{Mod(\x+2,4)+4*Mod(\y,2)}
     \node[draw,rectangle,minimum size=0.6cm] at (\x,\y)  {\eight};
     \fi
}
% neighborhood
\def\neighborhood{--
  ++( 0,  -.5)-- ++( 1,  0)-- ++( 0, -.5 )-- ++(0, -.5)-- ++( 1, 0  )--
  ++( 0, -1.0)-- ++( 1,  0)-- ++( 0,  1.0)-- ++(1, 0  )-- ++( 0, 1.0)--
  ++( 1,  0  )-- ++( 0,  1)-- ++(-1,  0  )-- ++(0, 1  )-- ++(-1, 0  )--
  ++( 0,  1  )-- ++(-1,  0)-- ++( 0, -1  )--
  ++(-1,  0  )-- ++( 0, -1)-- ++(-1,  0  )-- ++(0, -.6)}

\pgfmathtruncatemacro\lchalf{\bksize / 2}


% main center neighborhood
\path[draw=red,  rounded corners=.1cm, shorten >=2pt, line width=.75mm]
(\lchalf - 2.5, \lchalf) \neighborhood;

\end{tikzpicture}



\end{center}
\caption{\label{fig:nn} }
\end{figure}


% GPU implementation

\begin{figure} 
\begin{center}

\begin{tikzpicture}[scale=0.35]

\pgfmathtruncatemacro\lcsize{\bksize * \bkcount}
\pgfmathtruncatemacro\lcsizem{\lcsize - 1}
\pgfmathtruncatemacro\lcsizeb{\lcsize - \bksize}


\draw[very thin, gray] (-.5, -.5) grid (\lcsize - .5, \lcsize - .5);

\foreach \x in {0, \bksize, ..., \lcsizeb}
  \foreach \y in {0, \bksize, ..., \lcsizeb} {
    \pgfmathtruncatemacro\n{Mod(\x/\bksize, 2)+2 * Mod(\y/\bksize, 2)+1}
    \draw[colorone, xshift=\x cm, yshift=\y cm, rounded corners=2pt]
      (-.4, -.4) rectangle
      node[red, fill opacity=0, anchor=center, font=\Huge\bfseries\sffamily]
      {\n} (\bksize - .6, \bksize - .6);
  }

\foreach \x in {0, ..., \lcsizem}
  \foreach \y in {0, ..., \lcsizem}{
 
    \path[draw=black, fill=white] (\x, \y) circle(.25);
}


\fillblockgray{0}{0}
\markpartition{0}{0}{0}
\fillblockgray{0}{2}
\markpartition{0}{2}{0}
\fillblockgray{2}{0}
\markpartition{2}{0}{0}
\fillblockgray{2}{2}
\markpartition{2}{2}{0}
\end{tikzpicture}
\end{center}
\caption{}
\end{figure}
On GPU we adopt the hierarchical scheme from ref.~\cite{weigel}
suitably modified to account for bigger neighborhood.  We first divide
the whole lattice in blocks of $32\times 32$ points. Then we start a
kernel that process every forth block (see figure~\ref{fig:nn}).  Each
block is assigned to a block of 128 threads. First we fetch the values of the fields from global to shared memory (including border points).  
After that each thread updates
one point from the first partition. Then after synchronization, next
partition is updated and so on. After processing all eight(2D) or 16
(3D) partitions the kernel writes the shared memory back into global
and new kernel is started processing next batch of blocks. 
Altogether in this
way we managed to achieve $0.13$ nanoseconds for single lattice field
update on \emph{NVIDIA GTX 470}, reaching around $430$ Gflops that is
$40\%$ of $1088$ Gflops peak performance of this device.

% CPU implementation
In order to provide unbiased CPU vs GPU speed up results we provide
multithreaded vectorized CPU implementation. It uses OpenMP for
parallel execution,  SSE/AVX and compiler vector extensions for
vectorization.  This implementation does mimic GPU SIMT execution
model.  The SIMD instructions are used to process four (SSE) or eight
(AVX) updates in parallel. We use partitions as on GPU but we use only
one level {\em i.e.} we do not partition the lattice into blocks.
However not all scalar \emph{x86} instructions have vector
counterparts.  In particular direct \emph{XMM} registers gather and
scatter and vectorized integer operations for full length AVX 256-bit
registers are missing, which makes impossible to port random number
generator from 128-bit SSE to 256-bit AVX. Initially planned for AVX
standard, these were postponed to AVX2 planned for 2013. As soon AVX2
capable CPU devices appear on the market we plan to revise our
evaluation.


Our CPU \emph{OpenMP} and \emph{SSE/AVX} implementation compiled via
GCC 4.4 or higher and running on \emph{Intel Core i5 2.5 Ghz} quad
core CPU presented $15\times$ performance boost comparing to single
threaded scalar code and $3.76$ nanoseconds for single lattice field
update.  Which gives the $15$ Gflops that is $\sim10\%$ of the  $160$
Gflops peak performance of tested i5 CPU. There is no significant
increase in performance while switching from SSE to AVX instructions.

This gives around $28\times$ advantage to GPU, which is noticeably
less than promised by many publications, however much higher that
comes from comparison of  tested i5
CPU to \emph{GTX 470}. This can be
traced back to 128-bit only \emph{Tausworthe} random number generator
implementation and inefficient vector store and load operations
(gather/scatter).

\begin{thebibliography}{9}
\bibitem{parisi} G.~Parisi ``Statistical Field Theory'' Chapter 5, Perseus Books Publishing (1998).
\bibitem{weigel} M.~Weigel, J. Comput. Phys. \textbf{231}, 3064 (2012).
\bibitem{howes_thomas07}
Lee Howes and David Thomas.
``{\em Efficient random number generation and application using {CUDA}.}''
In Hubert Nguyen, editor, {\em GPU Gems 3}, chapter~37. Addison
  Wesley, August 2007.
\end{thebibliography}

\end{document}
